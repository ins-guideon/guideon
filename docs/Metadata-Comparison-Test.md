# 메타데이터 적용 전후 비교 테스트

## 📋 테스트 목적

메타데이터를 프롬프트에 적용한 전후의 답변 생성 품질을 비교하여 메타데이터의 효과를 측정합니다.

### 배경

메타데이터는 질문 의도별로 구조화된 가이드라인과 답변 형식을 제공하는 프롬프트 엔지니어링 기법입니다. 본 테스트는 다음을 검증합니다:

1. **의도별 가이드라인 준수도 향상**: 각 질문 의도에 맞는 최적화된 답변 형식 적용
2. **답변 품질 향상**: 메타데이터 기반 가이드라인을 통해 더 구조화되고 일관된 답변 생성
3. **조항 인용 개수 증가**: 메타데이터 가이드라인에 따라 규정 조항을 더 많이 인용
4. **답변 구조화 개선**: 의도별 특화된 답변 형식으로 가독성 및 이해도 향상

### Few-shot과의 차이점

- **Few-shot**: 구체적인 예제를 통해 패턴 학습 (예시 기반)
- **메타데이터**: 구조화된 가이드라인을 통해 원칙 전달 (규칙 기반)

## 🎯 테스트 사항

### 1. 답변 생성 테스트 (RegulationSearchService)

#### 측정 항목
- **답변 품질 점수**: AnswerQualityEnhancer를 통한 품질 점수 (0.0 ~ 1.0)
  - 답변 길이 평가 (25%)
  - 규정 조항 참조 평가 (25%)
  - 부정적 표현 체크 (25%)
  - 구조화 및 가독성 평가 (25%)
- **조항 인용 개수**: 답변에 포함된 규정 조항 인용 개수
- **답변 길이**: 생성된 답변의 문자 수
- **응답 시간**: 답변 생성에 소요된 시간
- **의도별 답변 품질**: 각 의도에 맞는 최적화된 답변 형식 적용 여부

#### 테스트 질문 세트 (의도별)

**기준확인 (4개)**
1. "연차 휴가는 몇 일인가요?"
2. "경조금은 얼마나 받을 수 있나요?"
3. "출장 일비는 얼마인가요?"
4. "야근 수당은 시간당 얼마인가요?"

**절차설명 (4개)**
1. "출장비 신청은 어떻게 하나요?"
2. "연차휴가 신청 절차는 어떻게 되나요?"
3. "경조휴가 신청은 어떻게 하나요?"
4. "법인카드 발급 신청은 어떻게 하나요?"

**가능여부 (4개)**
1. "연차를 시간 단위로 사용할 수 있나요?"
2. "경조휴가를 연차로 대체할 수 있나요?"
3. "출장비를 선지급받을 수 있나요?"
4. "연차를 다음 해로 이월할 수 있나요?"

**예외상황 (4개)**
1. "경비 지급에서 예외가 되는 경우가 있나요?"
2. "연차 사용에 예외가 있나요?"
3. "출장비 정산에 예외가 있나요?"
4. "경조휴가 사용에 예외가 있나요?"

**계산방법 (4개)**
1. "퇴직금은 어떻게 계산하나요?"
2. "야근 수당은 어떻게 계산하나요?"
3. "연차 일수는 어떻게 계산하나요?"
4. "출장비는 어떻게 계산하나요?"

**권리의무 (4개)**
1. "직원의 권리와 의무는 무엇인가요?"
2. "회사의 권리와 의무는 무엇인가요?"
3. "직원이 휴가를 거부당할 수 있나요?"
4. "회사가 임금을 지급하지 않으면 어떻게 하나요?"

**정보조회 (4개)**
1. "경조사에 대한 규정을 알려줘"
2. "출장 규정에 대해 알려줘"
3. "법인카드 사용 규정을 알려줘"
4. "건강검진에 대해 알려줘"

### 2. 메타데이터 품질 검증

#### 검증 항목
- 의도별 메타데이터 존재 여부: 7개 의도 모두 메타데이터 존재 확인
- 가이드라인 완전성: 각 의도별 가이드라인이 4개 이상 포함되어 있는지
- 답변 형식 가이드 존재: 의도별 답변 형식 가이드가 명시되어 있는지
- 예상 키워드 및 관련 규정 유형: 메타데이터에 예상 키워드와 관련 규정 유형이 포함되어 있는지

## 🚀 테스트 실행 방법

### 전제 조건

1. **API 키 설정**
   ```bash
   # 환경변수 설정
   export GOOGLE_API_KEY=your_api_key_here
   
   # 또는 application.properties에 설정
   gemini.api.key=your_api_key_here
   ```

2. **프로젝트 빌드**
   ```bash
   mvn clean compile
   ```

### 실행 방법

#### 방법 1: 전체 테스트 실행
```bash
mvn test -Dtest=MetadataComparisonTest
```

#### 방법 2: 특정 테스트만 실행
```bash
# 답변 생성 비교 테스트만 실행
mvn test -Dtest=MetadataComparisonTest#testAnswerGenerationComparison

# 메타데이터 품질 검증만 실행
mvn test -Dtest=MetadataComparisonTest#testMetadataQuality
```

**주의**: 답변 생성 비교 테스트는 실제 LLM API 호출이 필요하므로 API 할당량을 확인하세요. 테스트는 Rate limit을 고려하여 질문 간 7초 간격으로 실행됩니다.

#### 방법 3: IDE에서 실행
- IntelliJ IDEA / Eclipse에서 `MetadataComparisonTest.java` 파일 열기
- 클래스명 옆의 실행 버튼 클릭 또는 개별 테스트 메서드 실행

### 테스트 결과 확인

테스트 실행 후 다음 파일들이 생성됩니다:

```
test-results/
├── metadata-comparison-before.json    # 메타데이터 적용 전 결과
├── metadata-comparison-after.json     # 메타데이터 적용 후 결과
└── metadata-comparison-report.json    # 비교 리포트
```

## 📊 테스트 결과 분석

### 결과 파일 구조

#### metadata-comparison-before.json / metadata-comparison-after.json
```json
{
  "testName": "메타데이터 적용 전/후 (답변 생성)",
  "version": "before/after",
  "timestamp": "2025-11-16T...",
  "results": [
    {
      "question": "질문 내용",
      "analysisResult": {
        "keywords": ["키워드1", "키워드2"],
        "regulationTypes": ["규정유형"],
        "intent": "의도",
        "searchQuery": "검색쿼리"
      },
      "answer": "생성된 답변",
      "responseTimeMs": 1234,
      "metadata": {
        "qualityScore": 0.85,
        "articleReferences": 2,
        "answerLength": 456,
        "intent": "기준확인"
      }
    }
  ],
  "summary": {
    "averageResponseTimeMs": 1500.0,
    "totalTests": 10,
    "averageAnswerLength": 500.0,
    "averageQualityScore": 0.82,
    "averageArticleReferences": 2.5,
    "intentDistribution": {
      "기준확인": 3,
      "절차설명": 2,
      ...
    }
  }
}
```

#### metadata-comparison-report.json
```json
{
  "beforeVersion": "before",
  "afterVersion": "after",
  "testName": "메타데이터 적용 전 (답변 생성)",
  "comparisonTimestamp": "2025-11-16T...",
  "responseTime": {
    "before": 1500.0,
    "after": 1550.0,
    "improvementPercent": -3.33
  },
  "answerGenerationComparison": {
    "averageAnswerLength": {
      "before": 450.0,
      "after": 520.0,
      "improvementPercent": 15.56
    },
    "averageQualityScore": {
      "before": 0.78,
      "after": 0.85,
      "improvement": 0.07
    },
    "averageArticleReferences": {
      "before": 1.8,
      "after": 2.5,
      "improvement": 0.7
    }
  },
  "questionComparisons": [
    {
      "question": "질문 내용",
      "intent": {
        "before": "기준확인",
        "after": "기준확인"
      },
      "answer": {
        "before": "답변 (메타데이터 없음)",
        "after": "답변 (메타데이터 포함)",
        "length": {
          "before": 400,
          "after": 480
        },
        "qualityScore": {
          "before": 0.75,
          "after": 0.88,
          "improvement": 0.13
        },
        "articleReferences": {
          "before": 1,
          "after": 3,
          "improvement": 2
        }
      }
    }
  ],
  "summaryComparison": {
    "before": { ... },
    "after": { ... }
  }
}
```

### 주요 측정 지표

#### 1. 답변 품질 점수 (Quality Score)
- **목표**: 메타데이터 적용 후 품질 점수 향상
- **기대 효과**: 0.05 ~ 0.15 점 향상
- **평가 기준**:
  - 0.9 이상: 매우 우수
  - 0.8 ~ 0.9: 우수
  - 0.7 ~ 0.8: 양호
  - 0.7 미만: 개선 필요

#### 2. 조항 인용 개수 (Article References)
- **목표**: 메타데이터 가이드라인에 따라 규정 조항을 더 많이 인용
- **기대 효과**: 평균 0.5 ~ 1.5개 증가
- **의도별 기대값**:
  - 기준확인: 2개 이상
  - 절차설명: 1개 이상
  - 계산방법: 1개 이상
  - 권리의무: 1개 이상

#### 3. 답변 길이 (Answer Length)
- **목표**: 메타데이터 가이드라인에 따라 더 상세하고 구조화된 답변 생성
- **기대 효과**: 10% ~ 20% 증가
- **최적 길이**: 200 ~ 500자 (의도에 따라 다름)

#### 4. 의도별 답변 품질
- **기준확인**: 구체적인 숫자, 금액, 기간 명시 여부
- **절차설명**: 단계별 순서 및 구조화 여부
- **가능여부**: 첫 문장에서 명확한 답변 여부
- **예외상황**: 일반 원칙과 예외 구분 여부
- **계산방법**: 계산 공식 및 예시 포함 여부
- **권리의무**: 권리와 의무 구분 여부
- **정보조회**: 종합적이고 구조화된 정보 제공 여부

## 📈 예상 결과

### 메타데이터 적용 효과

1. **답변 품질 점수 향상**
   - 메타데이터 가이드라인에 따라 더 구조화되고 일관된 답변 생성
   - 의도별 특화된 답변 형식 적용으로 품질 향상

2. **조항 인용 개수 증가**
   - 메타데이터 가이드라인에 "규정 조항을 반드시 언급하세요" 포함
   - 기준확인, 계산방법 등 의도에서 특히 효과적

3. **답변 구조화 개선**
   - 절차설명: 단계별 순서 명확화
   - 가능여부: 첫 문장에서 명확한 답변
   - 예외상황: 일반 원칙과 예외 구분

4. **의도별 답변 형식 일관성 향상**
   - 각 의도에 맞는 최적화된 답변 형식 적용
   - 가이드라인 준수로 일관된 품질 유지

### Few-shot과의 비교

| 측정 지표 | Few-shot 효과 | 메타데이터 효과 |
|----------|--------------|----------------|
| 답변 품질 점수 | 중간 (예시 기반 학습) | 높음 (규칙 기반 가이드) |
| 조항 인용 개수 | 중간 | 높음 (명시적 가이드라인) |
| 답변 구조화 | 높음 (예시 패턴 학습) | 높음 (구조화된 가이드라인) |
| 의도별 특화 | 중간 | 높음 (의도별 메타데이터) |

## 🔍 결과 해석 가이드

### 긍정적 결과
- 품질 점수 향상: 메타데이터가 답변 품질에 긍정적 영향
- 조항 인용 증가: 가이드라인 준수도 향상
- 답변 길이 증가: 더 상세하고 구조화된 답변 생성

### 개선 필요 사항
- 품질 점수 변화 없음: 메타데이터 가이드라인 개선 필요
- 조항 인용 감소: 가이드라인 명확성 향상 필요
- 응답 시간 증가: 프롬프트 길이 최적화 필요

### 의도별 분석
- 특정 의도에서 효과가 낮은 경우: 해당 의도의 메타데이터 가이드라인 재검토
- 모든 의도에서 효과가 있는 경우: 메타데이터 적용 성공

## 📝 결론

메타데이터는 Few-shot과 함께 사용할 때 상호 보완적인 효과를 제공합니다:

- **Few-shot**: 구체적인 예제를 통한 패턴 학습
- **메타데이터**: 구조화된 가이드라인을 통한 원칙 전달

두 기법을 함께 사용하면 더 높은 품질의 답변을 생성할 수 있습니다.

## 🔗 관련 문서

- [Few-shot 비교 테스트](./FewShot-Comparison-Test.md)
- [프롬프트 엔지니어링 가이드](./Prompt-Engineering-Guide.md)
- [답변 품질 평가 기준](./Answer-Quality-Criteria.md)

