package com.guideon.tool;

import com.guideon.analyzer.DictionaryLoader;
import com.guideon.analyzer.EnhancedKoreanAnalyzer;
import com.guideon.analyzer.SearchQueryAnalyzer;
import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.ko.KoreanTokenizer;
import org.apache.lucene.analysis.ko.dict.UserDictionary;
import org.apache.lucene.analysis.synonym.SynonymMap;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

/**
 * ê²½ì¡°ì‚¬ ê²€ìƒ‰ ë¬¸ì œ ì§„ë‹¨ ë„êµ¬
 *
 * ë¬¸ì œ: "ê²½ì¡°ì‚¬ì— ëŒ€í•œ ê·œì •ì„ ì•Œë ¤ì¤˜" ì§ˆë¬¸ ì‹œ ë³µë¦¬í›„ìƒë¹„ê·œì •ì„ ì°¾ì§€ ëª»í•¨
 *
 * ì§„ë‹¨ í•­ëª©:
 * 1. ì¿¼ë¦¬ í† í°í™” í™•ì¸
 * 2. ë¬¸ì„œ í† í°í™” í™•ì¸
 * 3. ë™ì˜ì–´ í™•ì¥ í™•ì¸
 * 4. ì‚¬ìš©ì ì‚¬ì „ ì ìš© í™•ì¸
 */
public class DiagnosticTool {
    private static final Logger logger = LoggerFactory.getLogger(DiagnosticTool.class);

    public static void main(String[] args) {
        System.out.println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
        System.out.println("â•‘  ê²½ì¡°ì‚¬ ê²€ìƒ‰ ë¬¸ì œ ì§„ë‹¨ ë„êµ¬ (Diagnostic Tool)              â•‘");
        System.out.println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        System.out.println();

        try {
            // ì‚¬ì „ ë¡œë“œ
            System.out.println("ğŸ“š Loading dictionaries...");
            UserDictionary userDict = DictionaryLoader.loadUserDictionary();
            SynonymMap synonymMap = DictionaryLoader.loadSynonyms();

            System.out.println("âœ“ User dictionary: " + (userDict != null ? "Loaded" : "Not loaded"));
            System.out.println("âœ“ Synonym map: " + (synonymMap != null ? "Loaded" : "Not loaded"));
            System.out.println();

            // Analyzer ìƒì„±
            EnhancedKoreanAnalyzer indexAnalyzer = new EnhancedKoreanAnalyzer(
                userDict,
                DictionaryLoader.loadStopWords(),
                synonymMap,
                KoreanTokenizer.DecompoundMode.MIXED
            );

            SearchQueryAnalyzer queryAnalyzer = new SearchQueryAnalyzer(
                userDict,
                synonymMap,
                KoreanTokenizer.DecompoundMode.MIXED
            );

            // í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
            String[] queries = {
                "ê²½ì¡°ì‚¬ì— ëŒ€í•œ ê·œì •ì„ ì•Œë ¤ì¤˜",
                "ê²½ì¡°ì‚¬",
                "ê²½ì¡°ì‚¬ ê·œì •",
                "ê²½ì¡°íœ´ê°€",
                "ê²½ì¡°ì‚¬íœ´ê°€"
            };

            String[] documents = {
                "ì œ21ì¡°(ê²½ì¡°íœ´ê°€) 1. ë³¸ì¸ ê²°í˜¼: 5ì¼ 2. ìë…€ ê²°í˜¼: 2ì¼",
                "ê²½ì¡°ì‚¬ë¹„ ì§€ì› ê·œì •",
                "ë³µë¦¬í›„ìƒë¹„ê·œì • ê²½ì¡°ê¸ˆ ì§€ê¸‰ ê¸°ì¤€"
            };

            System.out.println("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
            System.out.println("ğŸ“Š QUERY TOKENIZATION ANALYSIS");
            System.out.println("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
            System.out.println();

            for (String query : queries) {
                List<String> tokens = analyze(queryAnalyzer, query);
                System.out.println("Query: " + query);
                System.out.println("Tokens: " + tokens);
                System.out.println("Token count: " + tokens.size());
                System.out.println("-----------------------------------------------------------");
            }

            System.out.println();
            System.out.println("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
            System.out.println("ğŸ“„ DOCUMENT TOKENIZATION ANALYSIS");
            System.out.println("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
            System.out.println();

            for (String doc : documents) {
                List<String> tokens = analyze(indexAnalyzer, doc);
                System.out.println("Document: " + doc);
                System.out.println("Tokens: " + tokens);
                System.out.println("Token count: " + tokens.size());
                System.out.println("-----------------------------------------------------------");
            }

            System.out.println();
            System.out.println("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
            System.out.println("ğŸ” MATCHING ANALYSIS");
            System.out.println("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
            System.out.println();

            String testQuery = "ê²½ì¡°ì‚¬ì— ëŒ€í•œ ê·œì •ì„ ì•Œë ¤ì¤˜";
            List<String> queryTokens = analyze(queryAnalyzer, testQuery);
            System.out.println("Test Query: " + testQuery);
            System.out.println("Query Tokens: " + queryTokens);
            System.out.println();

            for (String doc : documents) {
                List<String> docTokens = analyze(indexAnalyzer, doc);
                List<String> matches = findMatches(queryTokens, docTokens);

                System.out.println("Document: " + doc);
                System.out.println("Document Tokens: " + docTokens);
                System.out.println("Matching Tokens: " + matches);
                System.out.println("Match Score: " + matches.size() + "/" + queryTokens.size());

                if (matches.isEmpty()) {
                    System.out.println("âš ï¸ NO MATCHES - Document won't be retrieved!");
                } else {
                    System.out.println("âœ“ Has matches - Document may be retrieved");
                }
                System.out.println("-----------------------------------------------------------");
            }

            System.out.println();
            System.out.println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
            System.out.println("â•‘  DIAGNOSIS SUMMARY                                       â•‘");
            System.out.println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
            System.out.println();

            // ì§„ë‹¨ ê²°ê³¼
            List<String> mainQueryTokens = analyze(queryAnalyzer, "ê²½ì¡°ì‚¬ì— ëŒ€í•œ ê·œì •ì„ ì•Œë ¤ì¤˜");
            List<String> docTokens = analyze(indexAnalyzer, "ì œ21ì¡°(ê²½ì¡°íœ´ê°€) 1. ë³¸ì¸ ê²°í˜¼: 5ì¼");

            boolean hasMatch = findMatches(mainQueryTokens, docTokens).size() > 0;

            System.out.println("Query: \"ê²½ì¡°ì‚¬ì— ëŒ€í•œ ê·œì •ì„ ì•Œë ¤ì¤˜\"");
            System.out.println("Query tokens: " + mainQueryTokens);
            System.out.println();
            System.out.println("Document: \"ì œ21ì¡°(ê²½ì¡°íœ´ê°€) 1. ë³¸ì¸ ê²°í˜¼: 5ì¼\"");
            System.out.println("Document tokens: " + docTokens);
            System.out.println();

            if (!hasMatch) {
                System.out.println("âŒ PROBLEM IDENTIFIED:");
                System.out.println();

                // ë¬¸ì œ ë¶„ì„
                if (mainQueryTokens.contains("ê²½ì¡°ì‚¬")) {
                    if (docTokens.contains("ê²½ì¡°íœ´ê°€") || docTokens.contains("ê²½ì¡°ì‚¬íœ´ê°€")) {
                        System.out.println("Issue: Synonym not working properly");
                        System.out.println("- Query has: ê²½ì¡°ì‚¬");
                        System.out.println("- Document has: ê²½ì¡°íœ´ê°€");
                        System.out.println("- Expected: Should match via synonym expansion");
                        System.out.println();
                        System.out.println("Solution: Check SynonymGraphFilter in analyzers");
                    } else {
                        System.out.println("Issue: Document doesn't contain expected tokens");
                    }
                } else {
                    System.out.println("Issue: Query tokenization removed 'ê²½ì¡°ì‚¬'");
                    System.out.println("- This might be due to stopword filtering");
                    System.out.println();
                    System.out.println("Solution: Ensure 'ê²½ì¡°ì‚¬' is not in stopwords");
                }
            } else {
                System.out.println("âœ… NO ISSUES DETECTED");
                System.out.println("Query and document should match via BM25/Vector search");
            }

            System.out.println();
            System.out.println("âœ… Diagnostic Complete!");
            System.out.println();

            // Cleanup
            indexAnalyzer.close();
            queryAnalyzer.close();

        } catch (Exception e) {
            logger.error("Error during diagnostic", e);
            e.printStackTrace();
        }
    }

    /**
     * Analyze text with given analyzer
     */
    private static List<String> analyze(Analyzer analyzer, String text) throws IOException {
        List<String> tokens = new ArrayList<>();

        try (TokenStream stream = analyzer.tokenStream("content", text)) {
            CharTermAttribute termAttr = stream.addAttribute(CharTermAttribute.class);
            stream.reset();

            while (stream.incrementToken()) {
                tokens.add(termAttr.toString());
            }

            stream.end();
        }

        return tokens;
    }

    /**
     * Find matching tokens between two lists
     */
    private static List<String> findMatches(List<String> list1, List<String> list2) {
        List<String> matches = new ArrayList<>();
        for (String token : list1) {
            if (list2.contains(token) && !matches.contains(token)) {
                matches.add(token);
            }
        }
        return matches;
    }
}
